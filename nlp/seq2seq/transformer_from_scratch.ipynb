{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Embedding \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module): \n",
    "    \n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None: \n",
    "        super().__init__()  \n",
    "        self.d_model = d_model  # size of model embedding\n",
    "        self.vocab_size = vocab_size \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) \n",
    "    \n",
    "    def forward(self, x): \n",
    "        \"\"\"\n",
    "            (batch, seq_len) -> (batch, seq_len, d_model) \n",
    "            embedding(vocab, d_model) -> maps indices to a d model dimensional vector. \n",
    "            * math.sqrt(self.d_model) -> scale the embedding by sqrt(d_model) \n",
    "        \"\"\"\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Encoding \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        # positional encoding for each token in the sequence has d_model dimensions.\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "        # Apply cosine to odd indicess\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerNormalization Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module): \n",
    "        \n",
    "        def __init__(self, d_model: int, eps: float = 1e-6) -> None: \n",
    "            super().__init__() \n",
    "            self.d_model = d_model \n",
    "            self.eps = eps \n",
    "            self.gamma = nn.Parameter(torch.ones(d_model)) \n",
    "            self.beta = nn.Parameter(torch.zeros(d_model)) \n",
    "        \n",
    "        def forward(self, x): \n",
    "            mean = x.mean(dim=-1, keepdim=True) # get mean \n",
    "            std = x.std(dim=-1, keepdim=True)   # get varianceb \n",
    "            # normalize \n",
    "            x = (x - mean) / (std + self.eps)\n",
    "            # scale and shift: y = gamma * x + beta\n",
    "            # gamma for scaling, beta for shifting\n",
    "            y = self.gamma * x + self.beta\n",
    "            return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeedForwardBlock Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module): \n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None: \n",
    "        super().__init__() \n",
    "        self.d_model = d_model \n",
    "        self.d_ff = d_ff \n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        self.linear1 = nn.Linear(d_model, d_ff) \n",
    "        self.linear2 = nn.Linear(d_ff, d_model) \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: \n",
    "        # just a simple fully connected feed forward network\n",
    "        x = self.linear1(x) \n",
    "        x = self.relu(x) \n",
    "        x = self.dropout(x) \n",
    "        x = self.linear2(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head attention:\n",
    "- gồm nhiều self-attention với hi vọng mỗi self attention nhìn ở nhiều đặc điểm khác nhau sẽ hiểu ngữ cảnh tốt hơn. \n",
    "- d_k = d_model / head \n",
    "    - d_k là số chiều mà 1 head (1 attention nhìn được trong d_model - dimension of feature vector)\n",
    "    - d_model : là số chiều của feature vector \n",
    "    - head: là số self attention được khởi tạo. \n",
    "    - VD : Tôi đang đi đâu đó \n",
    "         -  10   10  10  10  10   (mỗi từ được đại diện bởi vector 10 dimension - sử dụng 2 head)\n",
    "    - head1 5    5   5   5   5\n",
    "    - head2 5    5   5   5   5 \n",
    "    - -> gom lại rồi trả về shape như đầu vào.\n",
    "\n",
    "- self-attention : chắc năng chính là tổng hợp các feature tại từ đang xét với ngữ cảnh của các từ xung quanh \n",
    "- feedforwardblock : Suy diện (tăng tính biểu diễn của feature).\n",
    "\n",
    "#### Mask in multi head attention\n",
    "2 dạng mask được sử dụng : \n",
    "- padding mask : được dùng để đảm bảo rằng các padding tokens (khi mà chuẩn hóa input đầu với max input len, đảm bảo các câu trong batch sẽ có cùng len) không ảnh hưởng gì đến cơ chế attention \n",
    "- look-ahead mask (mask multi head attention - Causal mask): được dùng để đảm bảo trong quá trình đào tạo và suy luận mỗi VỊ TRÍ trong chuỗi chỉ có thể tham gia vào các vị trí trước đó và vị trí hiện tại chứ không liên quan đến bất kỳ vị trí nào trong tương lai.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module): \n",
    "    \n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None: \n",
    "        super().__init__()\n",
    "        self.d_model = d_model \n",
    "        self.h = h \n",
    "        assert d_model % h == 0, \"d_model must be divisible by h\" \n",
    "        \n",
    "        self.d_k = d_model // h \n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) \n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) \n",
    "        self.w_o = nn.Linear(self.h * self.d_k, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    @classmethod\n",
    "    def attention(self, query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1] \n",
    "        \n",
    "        # (batch, h, seq_len, d_k) -> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k) \n",
    "        # print(\"attention scores : \", attention_scores.shape)\n",
    "        if mask is not None: \n",
    "            # print(mask.shape)\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9) \n",
    "            # print(\"mask\")\n",
    "        attention_scores = attention_scores.softmax(dim=-1) \n",
    "        if dropout is not None: \n",
    "            attention_scores = dropout(attention_scores) \n",
    "        # (batch, h, seq_len, seq_len) @ (batch, h, seq_len, d_k) -> (batch, h, seq_len, d_k)\n",
    "        return (attention_scores @ value), attention_scores \n",
    "    \n",
    "    def forward(self, query, key, value, mask): \n",
    "        query = self.w_q(query)  # (batch, seq_len, d_model) * (batch, d_model, d_model)-> (batch, seq_len, d_model)\n",
    "        key = self.w_k(key)      # same \n",
    "        value = self.w_v(value)  # same\n",
    "        \n",
    "        # split into h heads d_model = h * d_k\n",
    "        # (batch, seq_len, d_model) -> (batch, seq_len, h, d_k) -> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], -1, self.h, self.d_k).transpose(1, 2) \n",
    "        key = key.view(key.shape[0], -1, self.h, self.d_k).transpose(1, 2) \n",
    "        value = value.view(value.shape[0], -1, self.h, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # apply attention \n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout) \n",
    "        \n",
    "        # combine all heads together \n",
    "        # (batch, h, seq_len, d_k) -> (batch, seq_len, h, d_k) -> (batch, seq_len, h * d_k) \n",
    "        # contiguous() -> make sure the tensor is stored in a contiguous chunk of memory\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "        \n",
    "        # Multiply by W_o to stabilize the output shape \n",
    "        return self.w_o(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResidualConnection\n",
    "- xử lý vấn đề vanishing gradients \n",
    "- được dùng mỗi khi qua lớp multi head attention hoặc feed-forward layer -> xong rồi sẽ xử dụng layer normalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module): \n",
    "    \n",
    "    def __init__(self, d_model: int, dropout: float) -> None: \n",
    "        super().__init__() \n",
    "        self.d_model = d_model \n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        self.norm = LayerNormalization(d_model)\n",
    "    \n",
    "    def forward(self, x, sublayer): \n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module): \n",
    "    \n",
    "    def __init__(self, multi_head_attention: MultiHeadAttentionBlock, feed_forward: FeedForwardBlock, dropout: float) -> None: \n",
    "        super().__init__() \n",
    "        self.multi_head_attention = multi_head_attention \n",
    "        self.feed_forward = feed_forward \n",
    "        self.residual_connection = ResidualConnection(multi_head_attention.d_model, dropout)\n",
    "        self.feed_forward_residual_connection = ResidualConnection(multi_head_attention.d_model, dropout)\n",
    "    \n",
    "    def forward(self, x, src_mask): \n",
    "        x = self.residual_connection(x, lambda x: self.multi_head_attention(x, x, x, src_mask))\n",
    "        x = self.residual_connection(x, self.feed_forward)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module): \n",
    "    \n",
    "    def __init__(self, layers: nn.ModuleList) -> None: \n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(layers[0].multi_head_attention.d_model)\n",
    "    \n",
    "    def forward(self, x, mask): \n",
    "        for layer in self.layers: \n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module): \n",
    "    \n",
    "    def __init__(self, multi_head_attention: MultiHeadAttentionBlock, feed_forward: FeedForwardBlock, dropout: float) -> None: \n",
    "        super().__init__() \n",
    "        self.multi_head_attention = multi_head_attention \n",
    "        self.feed_forward = feed_forward \n",
    "        self.residual_connection = ResidualConnection(multi_head_attention.d_model, dropout)\n",
    "        self.feed_forward_residual_connection = ResidualConnection(multi_head_attention.d_model, dropout)\n",
    "        \n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask): \n",
    "        x = self.residual_connection(x, lambda x: self.multi_head_attention(x, x, x, tgt_mask)) # mask multi head attention\n",
    "        x = self.residual_connection(x, lambda x: self.multi_head_attention(x, encoder_output, encoder_output, src_mask))   \n",
    "        x = self.residual_connection(x, self.feed_forward)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module): \n",
    "    \n",
    "    def __init__(self, layers: nn.ModuleList) -> None: \n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(layers[0].multi_head_attention.d_model)\n",
    "    \n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask): \n",
    "        for layer in self.layers: \n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProjectionLayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # (batch, seq_len, d_model)\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        # (batch, seq_len, d_model)\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int = 512, N: int = 6, h: int = 8, dropout: float = 0.1 , d_ff: int = 2048) -> Transformer: \n",
    "    # create embedding layer: \n",
    "    # src_embed = InputEmbeddings(d_model=d_model, vocab_size=src_vocab_size)\n",
    "    src_embed = InputEmbeddings(d_model=d_model, vocab_size=src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model=d_model, vocab_size=tgt_vocab_size) \n",
    "    \n",
    "    # create positional encoding:\n",
    "    src_pos = PositionalEncoding(d_model=d_model, seq_len=src_seq_len, dropout=dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model=d_model, seq_len=tgt_seq_len, dropout=dropout) \n",
    "    \n",
    "    # create encoder and decoder layers: \n",
    "    encoder_blocks = [] \n",
    "    for _ in range(N): \n",
    "        encoder_multi_head_attention = MultiHeadAttentionBlock(d_model=d_model, h=h, dropout=dropout) \n",
    "        feed_forward = FeedForwardBlock(d_model=d_model, d_ff=d_ff, dropout=dropout)\n",
    "        encoder_block = EncoderBlock(multi_head_attention=encoder_multi_head_attention, feed_forward=feed_forward, dropout=dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "    \n",
    "    decoder_blocks = [] \n",
    "    for _ in range(N): \n",
    "        decoder_multi_head_attention = MultiHeadAttentionBlock(d_model=d_model, h=h, dropout=dropout)\n",
    "        feed_forward = FeedForwardBlock(d_model=d_model, d_ff=d_ff, dropout=dropout)\n",
    "        decoder_block = DecoderBlock(multi_head_attention=decoder_multi_head_attention, feed_forward=feed_forward, dropout=dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "    \n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks)) \n",
    "    decoder = Decoder(nn.ModuleList(decoder_blocks)) \n",
    "    \n",
    "    # create projection layer:\n",
    "    projection_layer = ProjectionLayer(d_model=d_model, vocab_size=tgt_vocab_size)\n",
    "    transformer = Transformer(encoder=encoder, decoder=decoder, src_embed=src_embed, tgt_embed=tgt_embed, src_pos=src_pos, tgt_pos=tgt_pos, projection_layer=projection_layer) \n",
    "    \n",
    "    # initialize weights: \n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer cho dịch máy từ en -> vi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "en, vi = [], []\n",
    "en_path = '/home/hoang.minh.an/anhalu-data/learning/deep-learning-from-scratch/nlp/seq2seq/data/en_sents.txt' \n",
    "vi_path = '/home/hoang.minh.an/anhalu-data/learning/deep-learning-from-scratch/nlp/seq2seq/data/vi_sents.txt'\n",
    "\n",
    "with open(en_path, 'r') as f: \n",
    "    en = f.readlines() \n",
    "\n",
    "with open(vi_path, 'r') as f:\n",
    "    vi = f.readlines()\n",
    "\n",
    "print(f\"Len en : {len(en)}\") \n",
    "print(f\"Len vi : {len(vi)}\") \n",
    "print(en[0], vi[0])\n",
    "en = en[:50000] \n",
    "vi = vi[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "UNK_token = 3 \n",
    "\n",
    "class Lang: \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"SOS\": 1, \"EOS\": 2, \"PAD\": 0, \"UNK\": 3}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {1: \"SOS\", 2: \"EOS\", 0: \"PAD\", 3: \"UNK\"}\n",
    "        self.n_words = 4  # Count SOS, EOS, PAD, UNK\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    \n",
    "    def get_idx(self, word):\n",
    "        if word not in self.word2index:\n",
    "            return self.word2index['UNK']\n",
    "        return self.word2index[word]\n",
    "            \n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = s.lower().strip()\n",
    "    # s = unicodeToAscii(s)\n",
    "    # s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    # s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def readLangs(lang1, lang2, en, vi, reverse=False):\n",
    "    pairs = []\n",
    "    for i in range(len(en)):\n",
    "        pairs.append([normalizeString(en[i]), normalizeString(vi[i])])\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "    \n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    \n",
    "    print(\"Count words in language:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)   \n",
    "    \n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "en_lang, vi_lang, pairs = readLangs('en', 'vi', en, vi, False)\n",
    "MAX_LENGTH = 60\n",
    "# print(MAX_LENGTH)\n",
    "\n",
    "# filter pair for max lenght \n",
    "pairs = [pair for pair in pairs if len(pair[0].split(\" \")) < MAX_LENGTH and len(pair[1].split(\" \")) < MAX_LENGTH] \n",
    "print(f\"Len pairs: {len(pairs)}\")\n",
    "print(pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2index(lang: Lang, sentence: str): \n",
    "    sentence = normalizeString(sentence)\n",
    "    res = [SOS_token] \n",
    "    for word in sentence.split(\" \"):\n",
    "        if len(word) == 0: \n",
    "            continue\n",
    "        res.append(lang.get_idx(word))\n",
    "    res.append(EOS_token)\n",
    "    return np.array(res)\n",
    "\n",
    "\n",
    "def get_dataloader(pairs, input_lang: Lang, output_lang: Lang, batch_size, max_lenght) -> DataLoader:\n",
    "    n = len(pairs)\n",
    "    input_pad = np.full((n, max_lenght), PAD_token, dtype=np.int32)\n",
    "    target_pad = np.full((n, max_lenght), PAD_token, dtype=np.int32)\n",
    "    for idx, pair in enumerate(pairs):\n",
    "        input = word2index(input_lang, pair[0])\n",
    "        target = word2index(output_lang, pair[1])\n",
    "        input_pad[idx, :len(input)] = input\n",
    "        target_pad[idx, :len(target)] = target\n",
    "    input_tensor = torch.tensor(input_pad, dtype=torch.long, device=device)\n",
    "    target_tensor = torch.tensor(target_pad, dtype=torch.long, device=device)\n",
    "    \n",
    "    dataset = TensorDataset(input_tensor, target_tensor) \n",
    "    sampler = RandomSampler(dataset) \n",
    "    dataloader = DataLoader(dataset, sampler=sampler, batch_size=batch_size) \n",
    "    return dataloader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init hyperparameters\n",
    "MAX_LENGTH = MAX_LENGTH + 2 # add 2 for SOS and EOS token\n",
    "src_vocab_size = en_lang.n_words\n",
    "tgt_vocab_size = vi_lang.n_words\n",
    "print(f\"Src vocab size: {src_vocab_size}\")\n",
    "print(f\"Tgt vocab size: {tgt_vocab_size}\")\n",
    "src_seq_len = MAX_LENGTH\n",
    "tgt_seq_len = MAX_LENGTH\n",
    "d_model = 128\n",
    "N = 2 # number of encoder and decoder layers    \n",
    "h = 4 # number of attention heads\n",
    "dropout = 0.1\n",
    "d_ff = 128 # feed forward hidden layer size\n",
    "lr = 5e-4\n",
    "\n",
    "transformer = build_transformer(src_vocab_size=src_vocab_size, tgt_vocab_size=tgt_vocab_size, src_seq_len=src_seq_len, tgt_seq_len=tgt_seq_len, d_model=d_model, N=N, h=h, dropout=dropout, d_ff=d_ff).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_epochs = 20\n",
    "train_pairs, test_pairs = train_test_split(pairs, test_size=0.05, random_state=42) \n",
    "print(f\"Len train: {len(train_pairs)}\") \n",
    "print(f\"Len test: {len(test_pairs)}\")\n",
    "\n",
    "train_dataloader = get_dataloader(train_pairs, en_lang, vi_lang, batch_size=batch_size, max_lenght=MAX_LENGTH) \n",
    "test_dataloader = get_dataloader(test_pairs, en_lang, vi_lang, batch_size=batch_size, max_lenght=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, tolerance=5, min_delta=0):\n",
    "\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, train_loss, validation_loss):\n",
    "        if (validation_loss - train_loss) > self.min_delta:\n",
    "            self.counter +=1\n",
    "            if self.counter >= self.tolerance:  \n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(transformer.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-9) \n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_token) \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def train(transformer, dataloader, optimizer, criterion, num_epochs):\n",
    "    early_stopping = EarlyStopping(tolerance=5, min_delta=0.1)\n",
    "    for epoch in range(num_epochs): \n",
    "        transformer.train() \n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        epoch_loss = 0\n",
    "        for batch in tqdm(dataloader, \"transformer training\"):\n",
    "            src, tgt = batch\n",
    "            src, tgt = src.to(device), tgt.to(device)   # tgt : (batch, seq_len)\n",
    "            # remove the last token from target\n",
    "            tgt_input = tgt[:, :-1] # (batch, seq_len - 1)\n",
    "            # remove the first token from target -> for prediction the next token\n",
    "            tgt_output = tgt[:, 1:] # (batch, seq_len - 1)\n",
    "            # mask padding token for src (batch, 1, 1, seq_len)\n",
    "            src_mask = (src != 0).unsqueeze(-2).unsqueeze(-2).to(device)    # (batch, 1, 1, seq_len)\n",
    "            \n",
    "            \"\"\"\n",
    "            create target mask: \n",
    "            - mask padding token tgt_input != 0 \n",
    "            - mask future token \n",
    "                - create a upper triangular matrix of shape (1, seq_len, seq_len)\n",
    "                [[1, 1, 1, 1], \n",
    "                 [0, 1, 1, 1],\n",
    "                 [0, 0, 1, 1],\n",
    "                 [0, 0, 0, 1]]\n",
    "                 - transpose to boolean mask\n",
    "                 [[true, true, true, true],\n",
    "                  [false, true, true, true],\n",
    "                  [false, false, true, true],\n",
    "                  [false, false, false, true]]\n",
    "                 - for prediction the next token, we don't need to see the future token \n",
    "                token 1 : true, false, false, false\n",
    "                token 2 : true, true, false, false\n",
    "                token 3 : true, true, true, false\n",
    "                token 4 : true, true, true, true\n",
    "                \n",
    "                True : we can see the token\n",
    "                False : we can't see the token\n",
    "            \"\"\"\n",
    "            feature_mask = (torch.triu(torch.ones((1, tgt_input.size(1), tgt_input.size(1)), device=device)) == 1).to(device)\n",
    "            padding_mask = (tgt_input != 0).unsqueeze(-2).to(device)\n",
    "            tgt_mask = padding_mask & feature_mask\n",
    "            tgt_mask = tgt_mask.unsqueeze(1) # (batch, 1, seq_len - 1, seq_len - 1)\n",
    "            # zero the gradient\n",
    "            optimizer.zero_grad()\n",
    "            # src : (batch, seq_len) -> (batch, seq_len, d_model)\n",
    "            # src_mask : (batch, 1, seq_len)\n",
    "            encoder_output = transformer.encode(src, src_mask)\n",
    "            # tgt_input : (batch, seq_len - 1) -> (batch, seq_len - 1, d_model)\n",
    "            # tgt_mask : (batch, seq_len - 1, seq_len - 1)\n",
    "            # encoder_output : (batch, seq_len, d_model)\n",
    "            # k, v: encoder_output (batch, seq_len, d_model) \n",
    "            # q: tgt_input (batch, seq_len - 1, d_model) \n",
    "            # attention_scores : (batch, h, seq_len - 1, seq_len)\n",
    "            # x : (batch, seq_len - 1, d_model)\n",
    "            decoder_output = transformer.decode(encoder_output, src_mask, tgt_input, tgt_mask)\n",
    "            \n",
    "            output = transformer.project(decoder_output)\n",
    "            # output : (batch, seq_len - 1, vocab_size)\n",
    "            # tgt_output : (batch, seq_len - 1)\n",
    "            # output.view(-1, output.size(-1)) : (batch * seq_len - 1, vocab_size)\n",
    "            # tgt_output.view(-1) : (batch * seq_len - 1)\n",
    "            \n",
    "            loss = criterion(output.reshape(-1, output.size(-1)), tgt_output.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # get accuracy \n",
    "            probabilities = F.softmax(output, dim=-1)\n",
    "            # Calculate accuracy excluding padding tokens\n",
    "            predictions = probabilities.argmax(dim=-1)\n",
    "\n",
    "            non_pad_mask = tgt_output != 0\n",
    "\n",
    "            correct_predictions += (predictions[non_pad_mask] == tgt_output[non_pad_mask]).sum().item()\n",
    "            total_predictions += non_pad_mask.sum().item()\n",
    "\n",
    "            \n",
    "        train_accuracy = correct_predictions / total_predictions\n",
    "            \n",
    "        # evaluate(transformer, test_dataloader, criterion)\n",
    "        print(f\"Epoch {epoch} Loss: {epoch_loss / len(train_dataloader)}, accuracy: {train_accuracy}\")\n",
    "        eval_loss, accuracy = evaluate(transformer, test_dataloader, criterion)\n",
    "        early_stopping(train_loss=epoch_loss / len(train_dataloader), validation_loss=eval_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "\n",
    "def evaluate(transformer, dataloader, criterion): \n",
    "    transformer.eval()\n",
    "    eval_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, \"transformer evaluation\"):\n",
    "            src, tgt = batch\n",
    "            src, tgt = src.to(device), tgt.to(device)   # tgt : (batch, seq_len)\n",
    "            # remove the last token from target\n",
    "            tgt_input = tgt[:, :-1] # (batch, seq_len - 1)\n",
    "            # remove the first token from target -> for prediction the next token\n",
    "            tgt_output = tgt[:, 1:] # (batch, seq_len - 1)\n",
    "            # mask padding token for src (batch, 1, 1, seq_len)\n",
    "            src_mask = (src != 0).unsqueeze(-2).unsqueeze(-2).to(device)    # (batch, 1, 1, seq_len)\n",
    "            \n",
    "            feature_mask = (torch.triu(torch.ones((1, tgt_input.size(1), tgt_input.size(1)), device=device)) == 1).to(device)\n",
    "            padding_mask = (tgt_input != 0).unsqueeze(-2).to(device)\n",
    "            tgt_mask = padding_mask & feature_mask\n",
    "            tgt_mask = tgt_mask.unsqueeze(1) # (batch, 1, seq_len - 1, seq_len - 1)\n",
    "            \n",
    "            encoder_output = transformer.encode(src, src_mask)\n",
    "            decoder_output = transformer.decode(encoder_output, src_mask, tgt_input, tgt_mask)\n",
    "            output = transformer.project(decoder_output)\n",
    "            loss = criterion(output.reshape(-1, output.size(-1)), tgt_output.reshape(-1))\n",
    "            eval_loss += loss.item()\n",
    "            probabilities = F.softmax(output, dim=-1)\n",
    "            # Calculate accuracy excluding padding tokens\n",
    "            predictions = probabilities.argmax(dim=-1)\n",
    "            non_pad_mask = tgt_output != 0\n",
    "            correct_predictions += (predictions[non_pad_mask] == tgt_output[non_pad_mask]).sum().item()\n",
    "            total_predictions += non_pad_mask.sum().item()\n",
    "            \n",
    "    accuracy = correct_predictions / total_predictions       \n",
    "    print(f\"Eval loss: {eval_loss / len(dataloader)}, accuracy: {accuracy / len(dataloader)}\")\n",
    "    return eval_loss / len(dataloader), accuracy\n",
    "\n",
    "train(transformer, train_dataloader, optimizer, criterion, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformer.state_dict(), \"/home/hoang.minh.an/anhalu-data/learning/deep-learning-from-scratch/nlp/seq2seq/save_model/transformer_en2vi_v2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from path \n",
    "path = \"/home/hoang.minh.an/anhalu-data/learning/deep-learning-from-scratch/nlp/seq2seq/save_model/transformer_en2vi_v2.pth\"\n",
    "transformer.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_in_dataloader(model, dataloader): \n",
    "    correct_predictions_all = 0\n",
    "    total_predictions_all = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, \"transformer evaluation\"):\n",
    "            src, tgt = batch\n",
    "            src, tgt = src.to(device), tgt.to(device)   # tgt : (batch, seq_len)\n",
    "            # remove the last token from target\n",
    "            tgt_input = tgt[:, :-1] # (batch, seq_len - 1)\n",
    "            # remove the first token from target -> for prediction the next token\n",
    "            tgt_output = tgt[:, 1:] # (batch, seq_len - 1)\n",
    "            # mask padding token for src (batch, 1, 1, seq_len)\n",
    "            src_mask = (src != 0).unsqueeze(-2).unsqueeze(-2).to(device)    # (batch, 1, 1, seq_len)\n",
    "            \n",
    "            feature_mask = (torch.triu(torch.ones((1, tgt_input.size(1), tgt_input.size(1)), device=device)) == 1).to(device)\n",
    "            padding_mask = (tgt_input != 0).unsqueeze(-2).to(device)\n",
    "            tgt_mask = padding_mask & feature_mask\n",
    "            tgt_mask = tgt_mask.unsqueeze(1) # (batch, 1, seq_len - 1, seq_len - 1)\n",
    "            \n",
    "            encoder_output = model.encode(src, src_mask)\n",
    "            decoder_output = model.decode(encoder_output, src_mask, tgt_input, tgt_mask)\n",
    "            output = model.project(decoder_output)\n",
    "            probabilities = F.softmax(output, dim=-1)\n",
    "            # Calculate accuracy excluding padding tokens\n",
    "            predictions = probabilities.argmax(dim=-1)\n",
    "            non_pad_mask = tgt_output != 0\n",
    "            \n",
    "            correct_predictions = (predictions[non_pad_mask] == tgt_output[non_pad_mask]).sum().item()\n",
    "            total_predictions = non_pad_mask.sum().item() \n",
    "\n",
    "            correct_predictions_all += correct_predictions\n",
    "            total_predictions_all += total_predictions\n",
    "            # print(predictions[0])\n",
    "            # print(tgt_output[0])\n",
    "            # break\n",
    "            \n",
    "    accuracy = correct_predictions_all / total_predictions_all  \n",
    "    print(\"Correct predictions: \", correct_predictions_all)\n",
    "    print(\"Total predictions: \", total_predictions_all)\n",
    "    print(\"Accuracy : \", accuracy)\n",
    "    return accuracy\n",
    "\n",
    "a = get_accuracy_in_dataloader(transformer, test_dataloader)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
