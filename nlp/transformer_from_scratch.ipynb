{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Embedding \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module): \n",
    "    \n",
    "    def __inti__(self, d_model: int, vocab_size: int) -> None: \n",
    "        super().__init__()  \n",
    "        self.d_model = d_model  # size of model embedding\n",
    "        self.vocab_size = vocab_size \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) \n",
    "    \n",
    "    def forward(self, x): \n",
    "        \"\"\"\n",
    "            (batch, seq_len) -> (batch, seq_len, d_model) \n",
    "            embedding(vocab, d_model) -> maps indices to a d model dimensional vector. \n",
    "            * math.sqrt(self.d_model) -> scale the embedding by sqrt(d_model) \n",
    "        \"\"\"\n",
    "        return self.embedding(x) * math.sqrt(self.d_model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Encoding \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module): \n",
    "    \n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model \n",
    "        self.seq_len = seq_len \n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        # positional encoding for each token in the sequence has d_model dimensions.\n",
    "        self.pe = torch.zeros(seq_len, d_model) \n",
    "        self.position = torch.arange(0, seq_len, dtype=torch.float()).unsqueeze(1) # (seq_len, 1) \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        self.pe[:, 0::2] = torch.sin(self.position * div_term)\n",
    "        self.pe[:, 1::2] = torch.cos(self.position * div_term)\n",
    "        self.pe = self.pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', self.pe) \n",
    "    \n",
    "    def forward(self, x): \n",
    "        x = x + self.pe[:, : x.shape[1], :].requires_grad(False)   # (batch, seq_len, d_model)\n",
    "        return self.dropout(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerNormalization Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module): \n",
    "        \n",
    "        def __init__(self, d_model: int, eps: float = 1e-6) -> None: \n",
    "            super().__init__() \n",
    "            self.d_model = d_model \n",
    "            self.eps = eps \n",
    "            self.gamma = nn.Parameter(torch.ones(d_model)) \n",
    "            self.beta = nn.Parameter(torch.zeros(d_model)) \n",
    "        \n",
    "        def forward(self, x): \n",
    "            mean = x.mean(dim=-1, keepdim=True) # get mean \n",
    "            std = x.std(dim=-1, keepdim=True)   # get varianceb \n",
    "            # normalize \n",
    "            x = (x - mean) / (std + self.eps)\n",
    "            # scale and shift: y = gamma * x + beta\n",
    "            # gamma for scaling, beta for shifting\n",
    "            y = self.gamma * x + self.beta\n",
    "            return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeedForwardBlock Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module): \n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None: \n",
    "        super().__init__() \n",
    "        self.d_model = d_model \n",
    "        self.d_ff = d_ff \n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        self.linear1 = nn.Linear(d_model, d_ff) \n",
    "        self.linear2 = nn.Linear(d_ff, d_model) \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: \n",
    "        # just a simple fully connected feed forward network\n",
    "        x = self.linear1(x) \n",
    "        x = self.relu(x) \n",
    "        x = self.dropout(x) \n",
    "        x = self.linear2(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head attention:\n",
    "- gồm nhiều self-attention với hi vọng mỗi self attention nhìn ở nhiều đặc điểm khác nhau sẽ hiểu ngữ cảnh tốt hơn. \n",
    "- d_k = d_model / head \n",
    "    - d_k là số chiều mà 1 head (1 attention nhìn được trong d_model - dimension of feature vector)\n",
    "    - d_model : là số chiều của feature vector \n",
    "    - head: là số self attention được khởi tạo. \n",
    "    - VD : Tôi đang đi đâu đó \n",
    "         -  10   10  10  10  10   (mỗi từ được đại diện bởi vector 10 dimension - sử dụng 2 head)\n",
    "    - head1 5    5   5   5   5\n",
    "    - head2 5    5   5   5   5 \n",
    "    - -> gom lại rồi trả về shape như đầu vào.\n",
    "\n",
    "- self-attention : chắc năng chính là tổng hợp các feature tại từ đang xét với ngữ cảnh của các từ xung quanh \n",
    "- feedforwardblock : Suy diện (tăng tính biểu diễn của feature).\n",
    "\n",
    "#### Mask in multi head attention\n",
    "2 dạng mask được sử dụng : \n",
    "- padding mask : được dùng để đảm bảo rằng các padding tokens (khi mà chuẩn hóa input đầu với max input len, đảm bảo các câu trong batch sẽ có cùng len) không ảnh hưởng gì đến cơ chế attention \n",
    "- look-ahead mask (mask multi head attention - Causal mask): được dùng để đảm bảo trong quá trình đào tạo và suy luận mỗi VỊ TRÍ trong chuỗi chỉ có thể tham gia vào các vị trí trước đó và vị trí hiện tại chứ không liên quan đến bất kỳ vị trí nào trong tương lai.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module): \n",
    "    \n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None: \n",
    "        super().__init__()\n",
    "        self.d_model = d_model \n",
    "        self.h = h \n",
    "        assert d_model % h == 0, \"d_model must be divisible by h\" \n",
    "        \n",
    "        self.d_k = d_model // h \n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) \n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) \n",
    "        self.w_o = nn.Linear(self.h * self.d_k, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    @classmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1] \n",
    "        \n",
    "        # (batch, h, seq_len, d_k) -> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k) \n",
    "        \n",
    "        if mask is not None: \n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9) \n",
    "        attention_scores = attention_scores.softmax(dim=-1) \n",
    "        if dropout is not None: \n",
    "            attention_scores = dropout(attention_scores) \n",
    "        # (batch, h, seq_len, seq_len) @ (batch, h, seq_len, d_k) -> (batch, h, seq_len, d_k)\n",
    "        return (attention_scores @ value), attention_scores \n",
    "    \n",
    "    def forward(self, query, key, value, mask): \n",
    "        query = self.w_q(query)  # (batch, seq_len, d_model) * (batch, d_model, d_model)-> (batch, seq_len, d_model)\n",
    "        key = self.w_k(key)      # same \n",
    "        value = self.w_v(value)  # same\n",
    "        \n",
    "        # split into h heads d_model = h * d_k\n",
    "        # (batch, seq_len, d_model) -> (batch, seq_len, h, d_k) -> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], -1, self.h, self.d_k).transpose(1, 2) \n",
    "        key = key.view(key.shape[0], -1, self.h, self.d_k).transpose(1, 2) \n",
    "        value = value.view(value.shape[0], -1, self.h, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # apply attention \n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout) \n",
    "        \n",
    "        # combine all heads together \n",
    "        # (batch, h, seq_len, d_k) -> (batch, seq_len, h, d_k) -> (batch, seq_len, h * d_k) \n",
    "        # contiguous() -> make sure the tensor is stored in a contiguous chunk of memory\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "        \n",
    "        # Multiply by W_o to stabilize the output shape \n",
    "        return self.w_o(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResidualConnection\n",
    "- xử lý vấn đề vanishing gradients \n",
    "- được dùng mỗi khi qua lớp multi head attention hoặc feed-forward layer -> xong rồi sẽ xử dụng layer normalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module): \n",
    "    \n",
    "    def __init__(self, d_model: int, dropout: float) -> None: \n",
    "        super().__init__() \n",
    "        self.d_model = d_model \n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        self.norm = LayerNormalization(d_model)\n",
    "    \n",
    "    def forward(self, x, sublayer): \n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module): \n",
    "    \n",
    "    def __init__(self, multi_head_attention: MultiHeadAttentionBlock, feed_forward: FeedForwardBlock, dropout: float) -> None: \n",
    "        super().__init__() \n",
    "        self.multi_head_attention = multi_head_attention \n",
    "        self.feed_forward = feed_forward \n",
    "        self.residual_connection = ResidualConnection(multi_head_attention.d_model, dropout)\n",
    "        self.feed_forward_residual_connection = ResidualConnection(multi_head_attention.d_model, dropout)\n",
    "    \n",
    "    def forward(self, x, src_mask): \n",
    "        x = self.residual_connection(x, lambda x: self.multi_head_attention(x, x, x, src_mask))\n",
    "        x = self.residual_connection(x, self.feed_forward)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module): \n",
    "    \n",
    "    def __init__(self, layers: nn.ModuleList) -> None: \n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(layers[0].multi_head_attention.d_model)\n",
    "    \n",
    "    def forward(self, x, mask): \n",
    "        for layer in self.layers: \n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module): \n",
    "    \n",
    "    def __init__(self, multi_head_attention: MultiHeadAttentionBlock, feed_forward: FeedForwardBlock, dropout: float) -> None: \n",
    "        super().__init__() \n",
    "        self.multi_head_attention = multi_head_attention \n",
    "        self.feed_forward = feed_forward \n",
    "        self.residual_connection = ResidualConnection(multi_head_attention.d_model, dropout)\n",
    "        self.feed_forward_residual_connection = ResidualConnection(multi_head_attention.d_model, dropout)\n",
    "        \n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask): \n",
    "        x = self.residual_connection(x, lambda x: self.multi_head_attention(x, x, x, tgt_mask)) # mask multi head attention\n",
    "        x = self.residual_connection(x, lambda x: self.multi_head_attention(x, encoder_output, encoder_output, src_mask))   \n",
    "        x = self.residual_connection(x, self.feed_forward)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module): \n",
    "    \n",
    "    def __init__(self, layers: nn.ModuleList) -> None: \n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(layers[0].multi_head_attention.d_model)\n",
    "    \n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask): \n",
    "        for layer in self.layers: \n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProjectionLayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # (batch, seq_len, d_model)\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        # (batch, seq_len, d_model)\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.projection_layer(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
