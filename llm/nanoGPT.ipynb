{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch \n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import math\n",
    "\n",
    "\n",
    "base_dir = Path(str(os.getcwd()))\n",
    "data_dir = base_dir / \"data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"\" \n",
    "for file in data_dir.glob(\"*.txt\"):\n",
    "    with open(file, \"r\") as f:\n",
    "        data += f.read()\n",
    "\n",
    "\n",
    "# create model predict on character level. \n",
    "vocab = \"\"\"aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\n \"\"\"\n",
    "\n",
    "# ignore characters that are not in the vocab\n",
    "data = \"\".join([char for char in data if char in vocab])\n",
    "\n",
    "vocab_len = len(vocab)\n",
    "char2idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx2char = {idx: char for char, idx in char2idx.items()}\n",
    "\n",
    "encoder = lambda x: [char2idx[char] for char in x]\n",
    "decoder = lambda x: \"\".join([idx2char[idx] for idx in x])\n",
    "\n",
    "vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "d_model = 512   # dimension of model : embedding size\n",
    "n_layers = 3   # number of layers\n",
    "n_heads = 4   # number of heads in multihead attention\n",
    "d_ff = 4*d_model     # dimension of feedforward network | 4 times d_model\n",
    "dropout = 0.1   # dropout rate\n",
    "max_len = 1024  # maximum length of input sequence\n",
    "\n",
    "# define hyperparameters and optimizer\n",
    "# n_epochs = 10  # number of epochs\n",
    "lr = 1e-4     # learning rate\n",
    "batch_size = 64  # batch size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "# Data : x, y -> x is input, y is output, y is the next character of x in the text\n",
    "\n",
    "ratio_train = 0.9\n",
    "data = torch.tensor(encoder(data), dtype=torch.long)\n",
    "\n",
    "train_data = data[:int(len(data) * ratio_train)]\n",
    "test_data = data[int(len(data) * ratio_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataset and dataloader. \n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx+self.seq_len]\n",
    "        y = self.data[idx+1:idx+self.seq_len+1]  # Shifted sequence\n",
    "        return x, y\n",
    "\n",
    "train_dataset = TextDataset(train_data, max_len)\n",
    "test_dataset = TextDataset(test_data, max_len)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 92, 118, 140,  ..., 140, 229,  36],\n",
       "        [ 74, 229,  38,  ..., 141, 136, 122],\n",
       "        [ 24, 172, 229,  ..., 191, 209, 209],\n",
       "        ...,\n",
       "        [207, 228,  43,  ...,  94, 229,  72],\n",
       "        [ 42,  64,  94,  ...,  14,  94,  70],\n",
       "        [148,  74, 207,  ..., 196, 228,  95]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the first sample from the train_loader in the batch first\n",
    "x, y = next(iter(train_loader)) # x, y are tensors of shape (batch_size, max_len)\n",
    "x\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module): \n",
    "    def __init__(self, vocab_len, n_embedding):\n",
    "        super(InputEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_len, n_embedding)\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        # positional encoding for each token in the sequence has d_model dimensions.\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "        # Apply cosine to odd indicess\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Linear transformation for queries, keys, and values\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Linear transformation for the concatenated outputs\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # register buffer for mask \n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(max_len, max_len)).view(1, 1, max_len, max_len))\n",
    "        \n",
    "    def mask_attention(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, dropout: nn.Dropout) -> torch.Tensor:\n",
    "        # Q, K, V: (batch, n_heads, seq_len, head_dim)\n",
    "        # mask: (batch, 1, seq_len, seq_len)\n",
    "        # computer attention score : Q * K^T / sqrt(d_k)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(Q.shape[-1]) # (batch, n_heads, seq_len, seq_len)\n",
    "        \n",
    "        # apply mask\n",
    "        # scores: (batch, n_heads, seq_len, seq_len)\n",
    "        mask_value = self.mask[:, :, :scores.size(2), :scores.size(3)] # (batch, 1, seq_len, seq_len)\n",
    "        scores = scores.masked_fill(mask_value == 0, float(\"-inf\"))\n",
    "        \n",
    "        # Apply softmax to the last dimension\n",
    "        attention = torch.softmax(scores, dim=-1)\n",
    "        if dropout is not None:\n",
    "            attention = dropout(attention)\n",
    "        \n",
    "        # Multiply the attention scores by the value vectors\n",
    "        # (batch, n_heads, seq_len, seq_len) * (batch, n_heads, seq_len, head_dim) -> (batch, n_heads, seq_len, head_dim)\n",
    "        output = torch.matmul(attention, V) \n",
    "        return output, attention \n",
    "    \n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        Q = self.w_q(x) # (batch, seq_len, d_model) * (batch, d_model, d_model) -> (batch, seq_len, d_model)\n",
    "        K = self.w_k(x)   # same \n",
    "        V = self.w_v(x) # same\n",
    "        \n",
    "        # Split the d_model (Q, k, V) dimension into n_heads\n",
    "        # d_model = head_dim * n_heads\n",
    "        \n",
    "        batch_size = Q.shape[0]\n",
    "        # (batch, seq_len, d_model) -> (batch, seq_len, n_heads, head_dim) -> (batch, n_heads, seq_len, head_dim)\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # apply attention\n",
    "        x, self.attention_scores = self.mask_attention(Q, K, V, self.dropout)\n",
    "        \n",
    "        # combine all heads together \n",
    "        # (batch, n_heads, seq_len, head_dim) -> (batch, seq_len, n_heads, head_dim) -> (batch, seq_len, d_model)\n",
    "        # contiguous() -> make sure the tensor is stored in a contiguous chunk of memory\n",
    "        x = x.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # apply projection\n",
    "        x = self.w_o(x)\n",
    "        return x\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class LayerNormalization(nn.Module): \n",
    "        \n",
    "    def __init__(self, d_model: int, eps: float = 1e-6) -> None: \n",
    "        super().__init__() \n",
    "        self.d_model = d_model \n",
    "        self.eps = eps \n",
    "        self.gamma = nn.Parameter(torch.ones(d_model)) \n",
    "        self.beta = nn.Parameter(torch.zeros(d_model)) \n",
    "    \n",
    "    def forward(self, x): \n",
    "        mean = x.mean(dim=-1, keepdim=True) # get mean \n",
    "        std = x.std(dim=-1, keepdim=True)   # get varianceb \n",
    "        # normalize \n",
    "        x = (x - mean) / (std + self.eps)\n",
    "        # scale and shift: y = gamma * x + beta\n",
    "        # gamma for scaling, beta for shifting\n",
    "        y = self.gamma * x + self.beta\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module): \n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float) -> None: \n",
    "        super().__init__() \n",
    "        self.attn = MaskedMultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ffwd = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = LayerNormalization(d_model)\n",
    "        self.norm2 = LayerNormalization(d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: \n",
    "        # norm + residual -> attention -> norm + residual -> feedforward -> norm + residual\n",
    "        x = self.attn(self.norm1(x) + x) \n",
    "        x = self.ffwd(self.norm2(x) + x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nanoGPT(nn.Module): \n",
    "    def __init__(self, vocab_len, d_model, n_layers, n_heads, d_ff, dropout) -> None: \n",
    "        super().__init__() \n",
    "        self.embedding = InputEmbedding(vocab_len, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
    "        self.layers = nn.ModuleList([Block(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_model, vocab_len)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: \n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers: \n",
    "            x = layer(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model and training script \n",
    "nano_gpt = nanoGPT(vocab_len, d_model, n_layers, n_heads, d_ff, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(nano_gpt.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1513/1513 [06:44<00:00,  3.74it/s]\n",
      "Epochs:   1%|          | 1/100 [06:44<11:06:49, 404.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.556898065851921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1513/1513 [06:44<00:00,  3.74it/s]\n",
      "Epochs:   2%|▏         | 2/100 [13:28<10:59:58, 404.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 1.9316366314494051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "def train_script(model, train_loader, loss_fn, optimizer, n_epochs, device): \n",
    "    model.to(device)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for epoch in tqdm(range(n_epochs), desc='Epochs'): \n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for x, y in tqdm(train_loader, desc='Batches'): \n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x) \n",
    "            loss = loss_fn(y_pred.view(-1, vocab_len), y.view(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "        \n",
    "        # evaluate the model on test data\n",
    "        if (epoch + 1) % 10 == 0: \n",
    "            test_loss = eval_script(model, test_loader, loss_fn, device)\n",
    "            test_losses.append(test_loss)\n",
    "            print(f\"Epoch {epoch+1}, Loss: {epoch_loss / len(train_loader)}, Test Loss: {test_loss}\")\n",
    "        \n",
    "            # if test loss is not decreasing, stop training\n",
    "            if len(test_losses) > 1 and test_losses[-1] >= test_losses[-2]: \n",
    "                print(\"Test loss is not decreasing anymore. Stop training.\")\n",
    "                break     \n",
    "            \n",
    "        else: \n",
    "            print(f\"Epoch {epoch+1}, Loss: {epoch_loss / len(train_loader)}\")\n",
    "    \n",
    "    # Plot the training and test loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(range(9, n_epochs, 10), test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def eval_script(model, test_loader, loss_fn, device): \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad(): \n",
    "        for x, y in test_loader: \n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred.view(-1, vocab_len), y.view(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(test_loader)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_epochs = 100\n",
    "\n",
    "nano_gpt = train_script(nano_gpt, train_loader, loss_fn, optimizer, n_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model \n",
    "torch.save(nano_gpt.state_dict(), \"nanoGPT.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16. Thúy Kiều là chị, em là Thúy Vân.\n",
      "Thương thì thấy thấy thương thương,\n",
      "Thôi thì thì thấy thấy thấy thương thay.\n",
      "Thôi thì thấ\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_text, max_len, device): \n",
    "    model.eval()\n",
    "    start_text = torch.tensor(encoder(start_text), dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for _ in range(max_len): \n",
    "            y_pred = model(start_text)\n",
    "            y_pred = y_pred[:, -1, :]\n",
    "            next_char = torch.argmax(y_pred, dim=-1).unsqueeze(0)\n",
    "            start_text = torch.cat([start_text, next_char], dim=1)\n",
    "\n",
    "            # check max length. \n",
    "            if len(start_text[0]) + 1 >= max_len:\n",
    "                break\n",
    "    return start_text\n",
    "\n",
    "start_text = '16. Thúy Kiều là chị, em là Thúy Vân.' \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generated_text = generate_text(nano_gpt, start_text, max_len, device)\n",
    "generated_text = generated_text.squeeze(0).cpu().numpy()\n",
    "generated_text = decoder(generated_text)\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
